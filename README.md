# Obfuscated Neural Network Lab

Основной компилируемый файл - `onnlab/onnlab.cpp`.

Проще всего запустить интересующий эксперимент следующим образом:

1. открыть проект (`onnlab/onnlab.vcxproj`) в Visual Studio (нужна поддержка C++20)
2. вписать в файл `onnlab/onnlab.cpp` в функцию `main` название интересующего эксперимента (список экспериментов приведён в файле `onnlab/onnlab.h`)
3. собрать и запустить проект

*Многие эксперименты можно настраивать - для этого нужно открыть файл эксперимента и поменять значения констант соответствующим образом (например, изменить количество потоков при запуске эксперимента).*

### Материалы демонстрации

Файлы из демонстрации #1:

- `exp_nm1ReLU_svsg4.cpp` - сеть nmReLU (немонотонный ReLU) с эквивалентом на ReLU
- `exp_nm1ilReLU_svsg2.cpp` - сеть nmiReLU (немонотонный "бессмертный" ReLU) с эквивалентом на ilReLU ("бессмертный" ограниченный ReLU) с метрикой spreadV1
- `exp_nm1ilReLU_svsg3.cpp` - сеть nmiReLU (немонотонный "бессмертный" ReLU) с эквивалентом на ilReLU ("бессмертный" ограниченный ReLU) с метрикой spreadV2

Файлы для получения данных демонстрации #2 (MNIST):

- `exp_nm1ilReLU_ReLU_mnist_cmp1.cpp` - майнер статистики обучения сетей ReLU-64 (конфигурация 1) и nmiReLU (конфигурация 2)
- `exp_nm1ilReLU_ReLU_mnist_CWL2_BIM_cmp1.cpp` - майнер статистики обучения и атак на несколько видов моделей, но в статье приведены данные только по обучению и атакам на ReLU-32 (конфигурация 3), а также nmiReLU (конфигурация 2). Помимо них, есть и другие интересные конфигурации обучения/атак, но в статье не хватило для них места. Очень кратко перескажу то, что не вошло в статью: обучение без эквивалента nmiReLU: точность ниже, чем с эквивалентом + обучение менее стабильное; обучение без эквивалента nmReLU: точность немного выше, чем nmiReLU без эквивалента, но ниже, чем с эквивалентом, а обучение всё равно идёт нестабильно; интересно, что устойчивость к атакам CWL2 у обучавшихся без эквивалента выше, а устойчивость к атакам BIM - наоборот ниже, чем у обучавшихся с эквивалентом.

### Дополнительная информация

`exp_nm1ReLU_svsg1.cpp` - реализация задачи из `exp_ReLU_svsg1` на `m1ReLU`. 

`exp_nm1ReLU_svsg3.cpp` - реализация задачи из `exp_ReLU_svsg1` (и `exp_m1ReLU_svsg1`) на `m1ReLU` без использования искусственного слоя, но с применением более качественной эвристики (средняя сходимость сетей должна возрасти).

`exp_nm1ReLU_svsg4.cpp` - реализация, аналогичная `exp_m1ReLU_svsg3`, но с инверсией результата, что положительно повлияло на использование преимуществ немонотонности.

`exp_iReLU_ReLU_mnist_cmp1.cpp` - бэнчмарк на основе MNIST для теста iReLU vs ReLU (по результатам они очень похожи, но iReLU как будто немного выше, нужно собрать больше статистики)

`exp_iReLU_mnist1.cpp` - тренировка iReLU на MNIST, референс для `exp_nm1ilReLU_mnist1.cpp` (ilReLU+nmiReLU) - по итогу: однослойная сеть - не самая лучшая среда (слишком "широкая", не "глубокая") для проявления пользы от немонотонности nmiReLU, результат в обоих тестах почти одинаков (+- шум).

В этом репозитории представлен проект `ONNLab`. Этот проект, по сути, - сборник экспериментов, построенный на базе общей структуры. Все эксперименты имеют расширение `.cpp`, реализации структурных элементов и др. - `.h`. 
*В некотором смысле, концепция проекта вдохновлена устройством большинства лабораторий оптики (а именно, оптическими столами) - как можно большая гибкость для самых разных экспериментов.*

Некоторые элементы выглядят недоделанными. По скольку проект зачастую сильно меняется, что-то может быть удалено или переделано, а в других файлах остаться незаконченные элементы. Я сохраняю всю историю в GIT, но не выгружаю в Public проекты GitHub.